\chapter*{Abstract}
\label{chap:abstract}
\addcontentsline{toc}{chapter}{\nameref{chap:abstract}}


Motor Imagery (MI)-based Brain-Computer Interfaces (BCIs) have shown promise for applications in neurorehabilitation and assistive technologies. However, accurately decoding MI from Electroencephalography (EEG) signals remains challenging due to the low signal-to-noise ratio and inter-subject variability. In this study, we propose a novel approach leveraging Self-Supervised Learning (SSL) to enhance MI classification using the OpenBMI dataset, which contains EEG signals from Steady-State Visual Evoked Potential (SSVEP) and MI tasks recorded from 54 subjects. Our key hypothesis is that learning semantic representations from SSVEP signals can improve the classification performance of MI tasks by capturing more generalizable brain activity patterns.


Our method involves training an SSL model to extract meaningful representations from both SSVEP and MI signals, followed by utilizing the learned embeddings for a supervised classification task on MI. We compare the performance of our SSL-enhanced features against baseline models trained on raw EEG signals without self-supervision. The evaluation includes classification accuracy, F1-score, and subject-independent generalization performance. By leveraging SSL, we aim to improve feature learning for MI tasks, potentially reducing the reliance on extensive labeled data while enhancing robustness across subjects. This approach paves the way for more efficient and transferable EEG-based BCI systems, ultimately contributing to advancements in brain-computer interaction and neurotechnology.


